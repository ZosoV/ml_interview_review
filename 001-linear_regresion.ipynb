{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d89cb15",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb85a70",
   "metadata": {},
   "source": [
    "Linear Regression is the basic ML model. It seeks to solve a regression problem. So that given a feature vector, we can predict a continuous output.\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta_0 + \\theta_1x\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta^T \\mathbf{x}\n",
    "$$\n",
    "\n",
    "The **objective** is to chose the parameters ($\\theta$) such that the hypothesis (predicted) $h_\\theta(x)$ is close to target $y$.\n",
    "\n",
    "To do that, we use a **loss function** to measure the error between the predicted output and the target output. The loss function used is a mean squared error (MSE).\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2\n",
    "$$\n",
    "\n",
    "In order to get a correct prediction, we need to minimize this loss using a Gradient Descent (GD) algorithm.\n",
    "\n",
    "$$\n",
    "\\min_\\theta J(\\theta) = \\min_\\theta \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0851b8ae",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "1. Start with some guesses about parameters $\\theta$.\n",
    "2. Find the gradient (derivatives) of $J(\\theta)$, which give us the direction to move towards. Roughly speaking, check around $360^{\\circ}$ for a direction to go down to a minimal point.\n",
    "3. Update the parameter in that direction to reduce the loss $J(\\theta)$.\n",
    "4. Keep changing the parameters until end up at a minimum or close to that.\n",
    "\n",
    "$$\n",
    "\\text{repeat until convergence} \\{ \\newline \\qquad \\qquad \\qquad \\quad \\theta_j := \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j} \\quad \\quad (\\text{for j=0,1,..,n})  \\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}\n",
    "$$\n",
    "\n",
    "The gradients indicate us the direction to move towards, and $\\alpha$ is the learning rate that indicate us how big step we take."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e8aa76",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"./illustrations/gradient_descent.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa73a9cf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51204bcc-2a83-449b-ba27-a171a6df300c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3094a7e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b7f99d-2d81-452e-a220-71698abfa3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9043881bd916c0b20ce2aa64cab75184f9f79aadaa8ce0db1d8955c67cfde88d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
